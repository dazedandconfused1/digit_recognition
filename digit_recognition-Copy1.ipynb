{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Deep Learning\n",
    "## Project: Build a Digit Recognition Program\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 1: Design and Test a Model Architecture\n",
    "Design and implement a deep learning model that learns to recognize sequences of digits. Train the model using synthetic data generated by concatenating character images from [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) or [MNIST](http://yann.lecun.com/exdb/mnist/). To produce a synthetic sequence of digits for testing, you can for example limit yourself to sequences up to five digits, and use five classifiers on top of your deep network. You would have to incorporate an additional ‘blank’ character to account for shorter number sequences.\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "- Your model can be derived from a deep neural net or a convolutional network.\n",
    "- You could experiment sharing or not the weights between the softmax classifiers.\n",
    "- You can also use a recurrent network in your deep neural net to replace the classification layers and directly emit the sequence of digits one-at-a-time.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42241.pdf). ([video](https://www.youtube.com/watch?v=vGPI_JvLoN0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import idx2numpy\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "import json\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train already present - Skipping extraction of train.tar.gz.\n",
      "test already present - Skipping extraction of test.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "#  Extracting Data\n",
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "train_filename = 'train.tar.gz'\n",
    "test_filename = 'test.tar.gz'\n",
    "#extra_filename = 'extra.tar.gz'-not downloaded extra for now\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "  if not os.path.exists(root+'/digitStruct.mat'):\n",
    "    print(\"digitStruct.mat is missing\")\n",
    "  return root+'/digitStruct.mat'\n",
    "  \n",
    "train_struct = maybe_extract(train_filename)\n",
    "test_struct = maybe_extract(test_filename)\n",
    "# extra_struct = maybe_extract(extra_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: (500, 28, 140)\n",
      "inputs: (60000, 28, 28)\n",
      "labels from idx2numpy: (60000,)\n",
      "sample label from idx2numpy: 5\n",
      "labels from converting idx2numpy to one-hot: (60000, 10)\n",
      "sample label from idx2numpy after conversion to one-hot: [0 0 0 0 0 1 0 0 0 0]\n",
      "One input's shape after concatenation: (28, 140)\n",
      "Corresponding label after concatenation: (array([0, 0, 0, 0, 0, 1, 0, 0, 0, 0]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))\n",
      "datatype of labels: <class 'list'>\n",
      "datatype of labels after conversion: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = idx2numpy.convert_from_file('train-images.idx3-ubyte')\n",
    "labels = idx2numpy.convert_from_file('train-labels.idx1-ubyte')\n",
    "labels_2= mnist.train.labels# these labels are in one-hot format\n",
    "dataset_size = 500# just for initial stages\n",
    "image_height = 28\n",
    "image_width = 140\n",
    "dataset = np.ndarray(shape=(dataset_size, image_height, image_width),\n",
    "                         dtype=np.float32)\n",
    "print (\"dataset:\",dataset.shape)\n",
    "data_labels = []#one-hot labels\n",
    "data_labels_2=[]#integer labels for display etc\n",
    "i = 0\n",
    "w = 0\n",
    "print (\"inputs:\",inputs.shape)\n",
    "print (\"labels from idx2numpy:\",labels.shape)\n",
    "print (\"sample label from idx2numpy:\",labels[0])\n",
    "# need to convert labels to one-hot\n",
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "labels_new = lb.fit_transform(labels)\n",
    "print (\"labels from converting idx2numpy to one-hot:\",labels_new.shape)\n",
    "print (\"sample label from idx2numpy after conversion to one-hot:\",labels_new[0])\n",
    "while i < dataset_size:\n",
    "    temp1 = np.hstack([inputs[w], inputs[w + 1], inputs[w + 2], inputs[w + 3], inputs[w + 4]])\n",
    "    dataset[i, :, :] = temp1\n",
    "    data_labels.append((labels_new[w], labels_new[w + 1], labels_new[w + 2], labels_new[w + 3], labels_new[w + 4]))\n",
    "    data_labels_2.append((labels[w], labels[w + 1], labels[w + 2], labels[w + 3], labels[w + 4]))\n",
    "    if i==0:\n",
    "        print(\"One input's shape after concatenation:\",temp1.shape)\n",
    "        print(\"Corresponding label after concatenation:\",data_labels[0])\n",
    "    w += 5\n",
    "    i += 1\n",
    "print (\"datatype of labels:\",type(data_labels))\n",
    "#to convert data_labels from list to array type\n",
    "from numpy import array\n",
    "data_labels=array(data_labels)\n",
    "print (\"datatype of labels after conversion:\",type(data_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABlCAYAAABdnhjZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH4ooKSjh1wOSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7303b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : (4, 4, 6, 7, 1)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def displaySequence(n):\n",
    "    fig=plt.figure()\n",
    "    plt.imshow(dataset[n])\n",
    "    plt.show()\n",
    "    print ('Label : {}'.format(data_labels_2[n]))\n",
    "displaySequence(random.randint(0, dataset_size))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABlCAYAAABdnhjZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRnsQ3gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc6f4e48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : (5, 0, 4, 1, 9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "y = data_labels\n",
    "X = dataset\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4)\n",
    "print(data_labels[0,1])\n",
    "displaySequence(0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape of training set: (350, 28, 140) (350, 5, 10)\n",
      "Training set after reshape: (350, 28, 140, 1) (350, 5, 10)\n"
     ]
    }
   ],
   "source": [
    "#reshaping so that 4d tensor can be obtained\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "print (\"initial shape of training set:\",X_train.shape,y_train.shape)\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size, image_size*5, num_channels)).astype(np.float32)\n",
    "  return dataset, labels\n",
    "X_train, y_train = reformat(X_train, y_train)\n",
    "print ('Training set after reshape:', X_train.shape, y_train.shape)\n",
    "X_test, y_test = reformat(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#making the CNN\n",
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_X_train = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_y_train = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "  tf.reshape(tf_y_train, [-1, 10])\n",
    "  #tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(X_test)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))# has depth 16\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))# again has depth 16\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [7* 7* depth, num_hidden], stddev=0.1))# fully connected layer, has dimensions 7*7, with 64 depth\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(# readout layer, 64 * number of labels\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "\n",
    "  def model(data):\n",
    "   # print(data.get_shape()) \n",
    "    conv = tf.nn.conv2d(data, layer1_weights, strides=[1, 2, 2, 1], padding='SAME')# stride=batch,ht,wdt,channels\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "  #  print(\"length of hidden layer afte conv1\",shape)\n",
    "    conv = tf.nn.conv2d(hidden, layer2_weights, strides=[1, 2, 2, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "  #  print(\"length of hidden layer after conv2\",shape)\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])# reshaping the conv layer to 7*7*64 for FC layer\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350, 28, 29, 1)\n",
      "(350, 10)\n"
     ]
    }
   ],
   "source": [
    "print (X_train[:,:,28:57].shape)\n",
    "print (y_train[:,1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(350, 28, 27, 1)\n",
      "(350, 10)\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'add_63:0' shape=(16, 10) dtype=float32>>\n",
      "<bound method Tensor.get_shape of <tf.Tensor 'strided_slice_4:0' shape=(16,) dtype=float32>>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimension 0 in both shapes must be equal, but are 16 and 1 for 'SoftmaxCrossEntropyWithLogits_3' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [16,10], [1,16].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\dinkar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    669\u001b[0m           \u001b[0mnode_def_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensors_as_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           status)\n\u001b[0m\u001b[1;32m    671\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dinkar\\Anaconda3\\envs\\tensorflow\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dinkar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 0 in both shapes must be equal, but are 16 and 1 for 'SoftmaxCrossEntropyWithLogits_3' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [16,10], [1,16].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-14b8bcc3bce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_y_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf_y_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf_y_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf_y_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf_y_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf_y_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[1;31m# Optimizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dinkar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(logits, labels, dim, name)\u001b[0m\n\u001b[1;32m   1447\u001b[0m   \u001b[1;31m# _CrossEntropyGrad() in nn_grad but not here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m   cost, unused_backprop = gen_nn_ops._softmax_cross_entropy_with_logits(\n\u001b[0;32m-> 1449\u001b[0;31m       precise_logits, labels, name=name)\n\u001b[0m\u001b[1;32m   1450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m   \u001b[1;31m# The output cost shape should be the input minus dim.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dinkar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36m_softmax_cross_entropy_with_logits\u001b[0;34m(features, labels, name)\u001b[0m\n\u001b[1;32m   2263\u001b[0m   \"\"\"\n\u001b[1;32m   2264\u001b[0m   result = _op_def_lib.apply_op(\"SoftmaxCrossEntropyWithLogits\",\n\u001b[0;32m-> 2265\u001b[0;31m                                 features=features, labels=labels, name=name)\n\u001b[0m\u001b[1;32m   2266\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0m_SoftmaxCrossEntropyWithLogitsOutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dinkar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    757\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    758\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dinkar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2240\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2241\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2242\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2243\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dinkar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1615\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1617\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1618\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32mC:\\Users\\dinkar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dinkar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[1;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dinkar\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension 0 in both shapes must be equal, but are 16 and 1 for 'SoftmaxCrossEntropyWithLogits_3' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [16,10], [1,16]."
     ]
    }
   ],
   "source": [
    "# Training- here we will feed 5 images 1 by 1\n",
    "print (X_train[:,:,29:56].shape)\n",
    "print (y_train[:,4].shape)\n",
    "logits1 = model(tf_X_train)\n",
    "logits2 = model(tf_X_train)\n",
    "logits3 = model(tf_X_train)\n",
    "logits4 = model(tf_X_train)\n",
    "logits5 = model(tf_X_train)\n",
    "print(logits1.get_shape)\n",
    "print(tf_y_train[:,0].get_shape)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits1,labels=tf_y_train[:,0]))+\\\n",
    "tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits2, labels=tf_y_train[:,1]))+\\\n",
    "tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits3, labels=tf_y_train[:,2]))+\\\n",
    "tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits4, labels=tf_y_train[:,3]))+\\\n",
    "tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits5, labels=tf_y_train[:,4]))\n",
    "# Optimizer.\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "#Predictions\n",
    "train_prediction = tf.pack([tf.nn.softmax(logits1),tf.nn.softmax(logits2),tf.nn.softmax(logits3),\\\n",
    "                            tf.nn.softmax(logits4),tf.nn.softmax(logits5)])\n",
    "test_prediction=tf.pack([tf.nn.softmax(model(tf_X_test[:,:,0:28])),\\\n",
    "                         tf.nn.softmax(model(tf_X_test[:,:,28:56])),\\\n",
    "                         tf.nn.softmax(model(tf_X_test[:,:,56:84])),\\\n",
    "                         tf.nn.softmax(model(tf_X_test[:,:,84:112])),\\\n",
    "                         tf.nn.softmax(model(tf_X_test[:,:,112:140]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "_What approach did you take in coming up with a solution to this problem?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "_How did you train your model? How did you generate your synthetic dataset?_ Include examples of images from the synthetic data you constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 2: Train a Model on a Realistic Dataset\n",
    "Once you have settled on a good architecture, you can train your model on real data. In particular, the [Street View House Numbers (SVHN)](http://ufldl.stanford.edu/housenumbers/) dataset is a good large-scale dataset collected from house numbers in Google Street View. Training on this more challenging dataset, where the digits are not neatly lined-up and have various skews, fonts and colors, likely means you have to do some hyperparameter exploration to perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "_Describe how you set up the training and testing data for your model. How does the model perform on a realistic dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "_What changes did you have to make, if any, to achieve \"good\" results? Were there any options you explored that made the results worse?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "_What were your initial and final results with testing on a realistic dataset? Do you believe your model is doing a good enough job at classifying numbers correctly?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 3: Test a Model on Newly-Captured Images\n",
    "\n",
    "Take several pictures of numbers that you find around you (at least five), and run them through your classifier on your computer to produce example results. Alternatively (optionally), you can try using OpenCV / SimpleCV / Pygame to capture live images from a webcam and run those through your classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "_Choose five candidate images of numbers you took from around you and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "_Is your model able to perform equally well on captured pictures or a live camera stream when compared to testing on the realistic dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Question 9\n",
    "_If necessary, provide documentation for how an interface was built for your model to load and classify newly-acquired images._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Leave blank if you did not complete this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Step 4: Explore an Improvement for a Model\n",
    "\n",
    "There are many things you can do once you have the basic classifier in place. One example would be to also localize where the numbers are on the image. The SVHN dataset provides bounding boxes that you can tune to train a localizer. Train a regression loss to the coordinates of the bounding box, and then test it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "_How well does your model localize numbers on the testing set from the realistic dataset? Do your classification results change at all with localization included?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "_Test the localization function on the images you captured in **Step 3**. Does the model accurately calculate a bounding box for the numbers in the images you found? If you did not use a graphical interface, you may need to investigate the bounding boxes by hand._ Provide an example of the localization created on a captured image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Optional Step 5: Build an Application or Program for a Model\n",
    "Take your project one step further. If you're interested, look to build an Android application or even a more robust Python program that can interface with input images and display the classified numbers and even the bounding boxes. You can for example try to build an augmented reality app by overlaying your answer on the image like the [Word Lens](https://en.wikipedia.org/wiki/Word_Lens) app does.\n",
    "\n",
    "Loading a TensorFlow model into a camera app on Android is demonstrated in the [TensorFlow Android demo app](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android), which you can simply modify.\n",
    "\n",
    "If you decide to explore this optional route, be sure to document your interface and implementation, along with significant results you find. You can see the additional rubric items that you could be evaluated on by [following this link](https://review.udacity.com/#!/rubrics/413/view)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your optional code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "Provide additional documentation sufficient for detailing the implementation of the Android application or Python program for visualizing the classification of numbers in images. It should be clear how the program or application works. Demonstrations should be provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your documentation here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \n",
    "**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
